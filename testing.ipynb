{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "# pip install ale-py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, capacity, file=None):\n",
    "        if file is not None:\n",
    "            try:\n",
    "                with open(file, \"rb\") as f:\n",
    "                    d = pickle.load(f)\n",
    "                    self.memory = d\n",
    "            except Exception as e:\n",
    "                self.memory = deque([], maxlen=capacity)\n",
    "                print(e)\n",
    "        else:\n",
    "            self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, el):\n",
    "        self.memory.append(el)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def to_list(self):\n",
    "        return list(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, channels, kernel, pool_kernel):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels[0], channels[1], kernel)\n",
    "        self.conv2 = nn.Conv2d(channels[1], channels[2], kernel)\n",
    "        self.pool_kernel = pool_kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, self.pool_kernel)\n",
    "        return x\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.double_conv1 = DoubleConv(channels=(3, 4, 8), kernel=(5,5), pool_kernel=(2,2))\n",
    "        self.double_conv2 = DoubleConv(channels=(8, 8, 16), kernel=(5,5), pool_kernel=(3,3))\n",
    "\n",
    "\n",
    "\n",
    "        self.layer1 = nn.Linear(144, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, 3)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(x, (3,3))\n",
    "        x = self.double_conv1(x)\n",
    "        x = self.double_conv2(x)\n",
    "        x = torch.flatten(x, x.dim()-3)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "\n",
    "def optimize_model(optimizer, memory, policy_net, target_net):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    state_batch = torch.tensor(np.array([state for (state, _, _, _) in transitions]), dtype=torch.float32)\n",
    "    action_batch = torch.tensor([[action] for (_, action, _, _,) in transitions])\n",
    "    reward_batch =  torch.tensor([reward for (_, _, reward, _) in transitions])\n",
    "    next_state_batch = torch.tensor(np.array([s2 for (_, _, _, s2) in transitions]), dtype=torch.float32)\n",
    "\n",
    "\n",
    "    state_qvalues = policy_net(state_batch)\n",
    "    state_action_values = state_qvalues.squeeze(1).gather(1, action_batch).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_qvalues = target_net(next_state_batch).squeeze(1)\n",
    "        next_state_values = next_qvalues.max(axis=1).values\n",
    "        next_state_values = torch.tensor(next_state_values)\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    target_net_state_dict = target_net.state_dict()\n",
    "    policy_net_state_dict = policy_net.state_dict()\n",
    "    for key in policy_net_state_dict:\n",
    "        target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "    target_net.load_state_dict(target_net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_picture(observation):\n",
    "    return observation[40:180, 10:-10, :]\n",
    "\n",
    "def find_pole_middle(observation):\n",
    "    for i in range(observation.shape[0]):\n",
    "        where = (observation[i, :, 0] < 100) & (observation[i, :, 1] < 100)\n",
    "        blue_pixels = sum(where)\n",
    "        if blue_pixels == 10:\n",
    "            return sum(np.where(where)[0])/10\n",
    "    return None\n",
    "\n",
    "def find_player_pos(observation):\n",
    "    for i in range(observation.shape[0]):\n",
    "        where = (observation[i, :, 1] < 100) & (observation[i, :, 2] < 100)\n",
    "        red_pixels = sum(where)\n",
    "        if red_pixels > 0:\n",
    "            return sum(np.where(where)[0])/red_pixels\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(optimizer, memory, policy_net, target_net, MAX_ITER=1000):\n",
    "    env = gym.make('ALE/Skiing-v5')\n",
    "    obs, info = env.reset()\n",
    "    obs = truncate_picture(obs)\n",
    "    obs = np.swapaxes(np.swapaxes(obs, 1, 2), 0, 1)\n",
    "    done = False\n",
    "    game_start = time.time()\n",
    "    action_sum, action_cnt = 0, 0\n",
    "    while not done and action_cnt < MAX_ITER:\n",
    "        s = time.time()\n",
    "        optimize_model(optimizer, memory, policy_net, target_net)\n",
    "        if random.random() < EPSILON:\n",
    "            res = target_net(torch.tensor(obs, dtype=torch.float32)).detach().numpy()\n",
    "            action = np.argmax(res)\n",
    "        else:\n",
    "            action = int(random.random()*A)\n",
    "        diff = time.time()-s\n",
    "        action_sum += (diff)\n",
    "        print(diff)\n",
    "        action_cnt += 1\n",
    "        for _ in range(ACTION_REPETITION):\n",
    "            observation, reward, done, trunc, info = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "        observation = truncate_picture(observation)\n",
    "        observation = np.swapaxes(np.swapaxes(observation, 1, 2), 0, 1)\n",
    "        memory.push((obs, action, reward, observation))\n",
    "        obs = observation\n",
    "\n",
    "    print(f\"Game done in {time.time()-game_start}s, avg it length {action_sum/action_cnt}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './memory'\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.95\n",
    "TAU = 0.005\n",
    "LR = 0.0005\n",
    "EPSILON = 0.8\n",
    "A = 3\n",
    "MODEL_PATH = \"./\"\n",
    "MEMORY_PATH = \"./memory\"\n",
    "ACTION_REPETITION = 5\n",
    "\n",
    "\n",
    "target_net = DQN()\n",
    "# target_net.load_state_dict(torch.load(MODEL_PATH+\"target\", weights_only=True, map_location=torch.device('cpu')))\n",
    "policy_net = DQN()\n",
    "policy_net.load_state_dict(target_net.state_dict())\n",
    "# policy_net.load_state_dict(torch.load(MODEL_PATH+\"policy\", weights_only=True, map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "\n",
    "memory = Memory(10**5, file=MEMORY_PATH)\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18785548210144043\n",
      "0.05125236511230469\n",
      "5.4836273193359375e-06\n",
      "0.0865011215209961\n",
      "5.7220458984375e-06\n",
      "0.054579973220825195\n",
      "0.12594318389892578\n",
      "0.1104578971862793\n",
      "1.049041748046875e-05\n",
      "0.09466242790222168\n",
      "0.09447121620178223\n",
      "0.1254284381866455\n",
      "0.07882213592529297\n",
      "0.171980619430542\n",
      "5.0067901611328125e-06\n",
      "0.15235400199890137\n",
      "0.09891724586486816\n",
      "0.10194659233093262\n",
      "0.09456253051757812\n",
      "0.08733439445495605\n",
      "0.09325814247131348\n",
      "5.4836273193359375e-06\n",
      "0.049184560775756836\n",
      "0.029987812042236328\n",
      "0.07764768600463867\n",
      "0.08049416542053223\n",
      "5.7220458984375e-06\n",
      "1.6689300537109375e-06\n",
      "0.11647295951843262\n",
      "0.07143616676330566\n",
      "0.08166098594665527\n",
      "6.67572021484375e-06\n",
      "0.029316425323486328\n",
      "0.1248941421508789\n",
      "0.1531846523284912\n",
      "0.15022754669189453\n",
      "0.09072113037109375\n",
      "0.1294090747833252\n",
      "0.10949158668518066\n",
      "0.14783072471618652\n",
      "6.9141387939453125e-06\n",
      "0.12464261054992676\n",
      "0.10724616050720215\n",
      "0.18843984603881836\n",
      "0.14593935012817383\n",
      "0.07376432418823242\n",
      "0.2135021686553955\n",
      "3.0994415283203125e-06\n",
      "0.17352771759033203\n",
      "0.14047908782958984\n",
      "0.08945345878601074\n",
      "0.11152410507202148\n",
      "0.061217308044433594\n",
      "0.06095719337463379\n",
      "0.08606815338134766\n",
      "0.10110640525817871\n",
      "4.76837158203125e-06\n",
      "4.0531158447265625e-06\n",
      "0.14258623123168945\n",
      "0.07561945915222168\n",
      "0.08121681213378906\n",
      "4.0531158447265625e-06\n",
      "0.10036706924438477\n",
      "0.10867714881896973\n",
      "6.4373016357421875e-06\n",
      "0.11341166496276855\n",
      "0.06925153732299805\n",
      "4.76837158203125e-06\n",
      "0.09798669815063477\n",
      "0.0735924243927002\n",
      "4.0531158447265625e-06\n",
      "0.10091853141784668\n",
      "0.14558649063110352\n",
      "0.10870599746704102\n",
      "0.16340112686157227\n",
      "0.1050562858581543\n",
      "0.0931389331817627\n",
      "5.0067901611328125e-06\n",
      "3.5762786865234375e-06\n",
      "0.08133435249328613\n",
      "0.06737160682678223\n",
      "0.07773518562316895\n",
      "0.06434106826782227\n",
      "0.04674410820007324\n",
      "0.10545158386230469\n",
      "0.08176994323730469\n",
      "0.09016871452331543\n",
      "0.08202290534973145\n",
      "0.08582067489624023\n",
      "0.10269594192504883\n",
      "0.1600790023803711\n",
      "0.17184019088745117\n",
      "0.1147465705871582\n",
      "0.17011022567749023\n",
      "0.14105701446533203\n",
      "0.11658191680908203\n",
      "0.09917950630187988\n",
      "0.09039020538330078\n",
      "1.1205673217773438e-05\n",
      "0.23162150382995605\n",
      "5.245208740234375e-06\n",
      "0.06843948364257812\n",
      "4.5299530029296875e-06\n",
      "0.15030193328857422\n",
      "0.08514738082885742\n",
      "4.76837158203125e-06\n",
      "0.08314967155456543\n",
      "0.07628178596496582\n",
      "0.07457184791564941\n",
      "0.051027536392211914\n",
      "5.0067901611328125e-06\n",
      "2.86102294921875e-06\n",
      "0.06970572471618652\n",
      "0.08282470703125\n",
      "5.0067901611328125e-06\n",
      "0.09009957313537598\n",
      "0.08919453620910645\n",
      "0.06973528861999512\n",
      "0.10746169090270996\n",
      "0.17118310928344727\n",
      "0.08182024955749512\n",
      "0.06537818908691406\n",
      "0.0008628368377685547\n",
      "2.1457672119140625e-06\n",
      "0.09216165542602539\n",
      "4.0531158447265625e-06\n",
      "0.17869257926940918\n",
      "0.1603538990020752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60264/3431084814.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state_values = torch.tensor(next_state_values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4178822040557861\n",
      "1.7779860496520996\n",
      "2.967149257659912\n",
      "1.169814109802246\n",
      "1.3975954055786133\n",
      "1.210216999053955\n",
      "1.048335075378418\n",
      "0.9691135883331299\n",
      "0.9340231418609619\n",
      "1.388777494430542\n",
      "1.349478006362915\n",
      "1.6445977687835693\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     play_game(optimizer, memory, policy_net, target_net)\n",
      "Cell \u001b[0;32mIn[76], line 11\u001b[0m, in \u001b[0;36mplay_game\u001b[0;34m(optimizer, memory, policy_net, target_net, MAX_ITER)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m action_cnt \u001b[38;5;241m<\u001b[39m MAX_ITER:\n\u001b[1;32m     10\u001b[0m     s \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 11\u001b[0m     optimize_model(optimizer, memory, policy_net, target_net)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m EPSILON:\n\u001b[1;32m     13\u001b[0m         res \u001b[38;5;241m=\u001b[39m target_net(torch\u001b[38;5;241m.\u001b[39mtensor(obs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[71], line 60\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m(optimizer, memory, policy_net, target_net)\u001b[0m\n\u001b[1;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(state_action_values, expected_state_action_values)\n\u001b[1;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 60\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     61\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(policy_net\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     62\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    play_game(optimizer, memory, policy_net, target_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./\"\n",
    "torch.save(target_net.state_dict(), MODEL_PATH+\"target\")\n",
    "torch.save(policy_net.state_dict(), MODEL_PATH+\"policy\")\n",
    "with open(MEMORY_PATH, 'wb') as f:\n",
    "    pickle.dump(memory.memory, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109619\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, policy_net.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 140, 3)\n",
      "Pole: 50.0\n",
      "Player: 64.5\n",
      "Pole: 50.0\n",
      "Player: 57.5\n",
      "Pole: 50.0\n",
      "Player: 51.5\n",
      "Pole: 80.0\n",
      "Player: 52.5\n",
      "Pole: 80.0\n",
      "Player: 61.5\n",
      "Pole: 80.0\n",
      "Player: 71.5\n",
      "Pole: 80.0\n",
      "Player: 82.5\n",
      "Pole: 88.0\n",
      "Player: 96.5\n",
      "Pole: 88.0\n",
      "Player: 107.5\n",
      "Pole: 88.0\n",
      "Player: 117.5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('ALE/Skiing-v5')\n",
    "env.reset()\n",
    "observation, reward, done, trunc, info = env.step(0)\n",
    "truncated_obs = observation[40:180, 10:-10, :]\n",
    "print(truncated_obs.shape)\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        observation, reward, done, trunc, info = env.step(int(random.random()*3))\n",
    "    truncated_obs = truncate_picture(observation)\n",
    "    print(f\"Pole: {find_pole_middle(truncated_obs)}\")\n",
    "    print(f\"Player: {find_player_pos(truncated_obs)}\")\n",
    "plt.imshow(truncated_obs, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
